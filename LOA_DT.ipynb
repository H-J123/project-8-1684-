{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5401da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "# ==============================================================\n",
    "# Lyrebird Optimization Algorithm (LOA)\n",
    "# for Feature Selection using Decision Tree on KC2 Dataset\n",
    "# Includes: Convergence Curve, Error Plot, ROC & AUC Comparison\n",
    "# ==============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# ==============================================================\n",
    "# Load and Prepare Dataset\n",
    "# ==============================================================\n",
    "\n",
    "# âœ… Change this path to your dataset location\n",
    "data = pd.read_csv(r\"/content/PC4_csv.csv\")\n",
    "\n",
    "print(\"âœ… Dataset Loaded Successfully!\")\n",
    "print(f\"Shape: {data.shape}\")\n",
    "print(data.head())\n",
    "\n",
    "# Separate target column\n",
    "y = data.iloc[:, -1]\n",
    "X = data.iloc[:, :-1]\n",
    "\n",
    "# Drop 'id' column if it exists\n",
    "if 'id' in X.columns:\n",
    "    X.drop('id', axis=1, inplace=True)\n",
    "\n",
    "# Normalize data\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "Dim = X_train.shape[1]\n",
    "\n",
    "# ==============================================================\n",
    "# Define Lyrebird Optimization Algorithm (LOA)\n",
    "# ==============================================================\n",
    "\n",
    "def fitness_function(features):\n",
    "    \"\"\"Fitness based on Decision Tree classification error\"\"\"\n",
    "    selected = np.where(features == 1)[0]\n",
    "    if len(selected) == 0:\n",
    "        return 1.0  # maximum error if no feature selected\n",
    "\n",
    "    clf = DecisionTreeClassifier(random_state=42)\n",
    "    clf.fit(X_train.iloc[:, selected], y_train)\n",
    "    pred = clf.predict(X_test.iloc[:, selected])\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    return 1 - acc  # minimize error\n",
    "\n",
    "def LOA_feature_selection(X_train, y_train, X_test, y_test, Dim, MaxIter=200, N=30):\n",
    "    \"\"\"Lyrebird Optimization Algorithm for Feature Selection\"\"\"\n",
    "    # Initialize population\n",
    "    X_pop = np.random.randint(0, 2, (N, Dim))\n",
    "    Fitness = np.zeros(N)\n",
    "\n",
    "    # Parameters\n",
    "    alpha, beta, gamma = 0.5, 0.8, 0.9\n",
    "\n",
    "    # Initialize best\n",
    "    Best_FF = np.inf\n",
    "    Best_P = np.zeros(Dim)\n",
    "\n",
    "    # Evaluate initial population\n",
    "    for i in range(N):\n",
    "        Fitness[i] = fitness_function(X_pop[i])\n",
    "        if Fitness[i] < Best_FF:\n",
    "            Best_FF = Fitness[i]\n",
    "            Best_P = X_pop[i].copy()\n",
    "\n",
    "    Conv_curve = np.zeros(MaxIter)\n",
    "    Avg_error_curve = np.zeros(MaxIter)\n",
    "\n",
    "    print(\"\\nðŸš€ LOA Optimization Started...\\n\")\n",
    "    for t in range(MaxIter):\n",
    "        errors = []\n",
    "        for i in range(N):\n",
    "            A = alpha * np.random.uniform(-1, 1, Dim)\n",
    "            B = beta * np.random.uniform(0, 1, Dim)\n",
    "            C = gamma * np.random.uniform(0, 1)\n",
    "\n",
    "            new_pos = X_pop[i] + A * (Best_P - B * X_pop[i]) + C * np.random.randn(Dim)\n",
    "            new_pos = 1 / (1 + np.exp(-new_pos))\n",
    "            new_pos = np.where(new_pos > 0.5, 1, 0)\n",
    "\n",
    "            new_fit = fitness_function(new_pos)\n",
    "            errors.append(new_fit)\n",
    "\n",
    "            # Greedy selection\n",
    "            if new_fit < Fitness[i]:\n",
    "                X_pop[i] = new_pos.copy()\n",
    "                Fitness[i] = new_fit\n",
    "\n",
    "            if new_fit < Best_FF:\n",
    "                Best_FF = new_fit\n",
    "                Best_P = new_pos.copy()\n",
    "\n",
    "        Conv_curve[t] = Best_FF\n",
    "        Avg_error_curve[t] = np.mean(errors)\n",
    "\n",
    "        if t % 20 == 0 or t == MaxIter - 1:\n",
    "            print(f\"Iteration {t+1}/{MaxIter} -> Best Fitness: {Best_FF:.4f}\")\n",
    "\n",
    "    print(\"\\nâœ… LOA Optimization Completed!\")\n",
    "    return Best_P, Best_FF, Conv_curve, Avg_error_curve\n",
    "\n",
    "# ==============================================================\n",
    "# Run LOA for Feature Selection\n",
    "# ==============================================================\n",
    "\n",
    "Best_features, Best_error, Conv_curve, Avg_error_curve = LOA_feature_selection(\n",
    "    X_train, y_train, X_test, y_test, Dim=Dim, MaxIter=150, N=30\n",
    ")\n",
    "\n",
    "selected_idx = np.where(Best_features == 1)[0]\n",
    "print(f\"\\nðŸŽ¯ Selected {len(selected_idx)} features out of {Dim}:\")\n",
    "print(list(X_train.columns[selected_idx]))\n",
    "\n",
    "# ==============================================================\n",
    "# Decision Tree Evaluation: With and Without Feature Selection\n",
    "# ==============================================================\n",
    "\n",
    "# Without Feature Selection\n",
    "clf_full = DecisionTreeClassifier(random_state=42)\n",
    "clf_full.fit(X_train, y_train)\n",
    "pred_full = clf_full.predict(X_test)\n",
    "prob_full = clf_full.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# With LOA-Selected Features\n",
    "clf_selected = DecisionTreeClassifier(random_state=42)\n",
    "clf_selected.fit(X_train.iloc[:, selected_idx], y_train)\n",
    "pred_sel = clf_selected.predict(X_test.iloc[:, selected_idx])\n",
    "prob_sel = clf_selected.predict_proba(X_test.iloc[:, selected_idx])[:, 1]\n",
    "\n",
    "# Metrics\n",
    "def evaluate_model(y_true, y_pred, y_prob):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, pos_label='Y')\n",
    "    rec = recall_score(y_true, y_pred, pos_label='Y')\n",
    "    f1 = f1_score(y_true, y_pred, pos_label='Y')\n",
    "    auc_score = roc_auc_score(y_true, y_prob)\n",
    "    return acc, prec, rec, f1, auc_score\n",
    "\n",
    "acc_full, prec_full, rec_full, f1_full, auc_full = evaluate_model(y_test, pred_full, prob_full)\n",
    "acc_sel, prec_sel, rec_sel, f1_sel, auc_sel = evaluate_model(y_test, pred_sel, prob_sel)\n",
    "\n",
    "print(\"\\nðŸ“Š Decision Tree Performance Comparison\")\n",
    "print(\"----------------------------------------------------\")\n",
    "print(f\"Without Feature Selection -> Accuracy: {acc_full:.4f}, AUC: {auc_full:.4f}\")\n",
    "print(f\"With LOA Feature Selection -> Accuracy: {acc_sel:.4f}, AUC: {auc_sel:.4f}\")\n",
    "print(f\"Without Feature Selection -> Prediction: {prec_full:.4f}, PRE:{prec_full:.4f}\")\n",
    "print(f\"With LOA Feature Selection -> Prediction : {prec_sel:.4f}, PRE: {prec_sel:.4f}\")\n",
    "print(f\"Without Feature Selection -> Recall: {rec_full:.4f}, REC: {rec_full:.4f}\")\n",
    "print(f\"With LOA Feature Selection -> Recall: {rec_sel:.4f}, REC: {rec_sel:.4f}\")\n",
    "print(f\"Without Feature Selection -> F1-score: {f1_full:.4f}, F1:{f1_full:.4f}\")\n",
    "print(f\"With LOA Feature Selection -> F1-score : {f1_sel:.4f}, F1: {f1_sel:.4f}\")\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# Visualization: Convergence, Error Curve, ROC Comparison\n",
    "# ==============================================================\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Convergence Curve\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(Conv_curve, 'b-', linewidth=2)\n",
    "plt.title(\"Convergence Curve (Best Fitness)\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Fitness (Error)\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Average Error Plot\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(Avg_error_curve, 'r-', linewidth=2)\n",
    "plt.title(\"Average Error per Iteration\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Average Error\")\n",
    "plt.grid(True)\n",
    "\n",
    "# ROC Curve Comparison\n",
    "fpr_full, tpr_full, _ = roc_curve(y_test, prob_full, pos_label='Y')\n",
    "fpr_sel, tpr_sel, _ = roc_curve(y_test, prob_sel, pos_label='Y')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(fpr_full, tpr_full, 'k--', label=f'Without FS (AUC={auc_full:.4f})')\n",
    "plt.plot(fpr_sel, tpr_sel, 'g-', label=f'With LOA FS (AUC={auc_sel:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.title(\"ROC Curve Comparison (Decision Tree)\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
